{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6874a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22cc91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Choose embedding & LLM backends ---\n",
    "# Option A: OpenAI for embeddings + OpenAI LLM (requires OPENAI_API_KEY)\n",
    "# Option B: sentence-transformers for embeddings + any LLM (or local LLM)\n",
    "\n",
    "USE_OPENAI = False   # set False to use sentence-transformers embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e27817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "PAPERS_DIR = Path(\"./papers\")\n",
    "VECTORSTORE_PATH = Path(\"./faiss_index\")  # directory to save vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885b8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Imports (LangChain style)\n",
    "# --------------------------\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "if USE_OPENAI:\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    from langchain.llms import OpenAI\n",
    "else:\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings  # sentence-transformers wrapper\n",
    "    # for non-OpenAI LLM, you could use e.g., HuggingFaceHub or local LLM integration\n",
    "    from langchain_groq import ChatGroq\n",
    " # placeholder - replace with a local LLM if needed\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b68aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1d301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs from papers\n",
      "Loaded 292 source documents (PDF files).\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1) Load PDFs from ./papers\n",
    "# --------------------------\n",
    "print(\"Loading PDFs from\", PAPERS_DIR)\n",
    "loader = PyPDFDirectoryLoader(str(PAPERS_DIR))  # loads each pdf as Document with metadata['source']\n",
    "raw_docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} source documents (PDF files).\")\n",
    "# Each raw_doc typically contains the whole PDF text in one Document object.\n",
    "# We'll split into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c383eec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 887 chunks.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2) Split documents into chunks\n",
    "# --------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "texts = splitter.split_documents(raw_docs)\n",
    "print(f\"Split into {len(texts)} chunks.\")\n",
    "\n",
    "# Each chunk keeps metadata including 'source' (filepath). That's important for citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d01fe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS index from disk...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3) Embed chunks & create VectorStore\n",
    "# --------------------------\n",
    "if USE_OPENAI:\n",
    "    # Requires OPENAI_API_KEY in env\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    # This uses sentence-transformers models under the hood.\n",
    "    # Common choices: \"all-MiniLM-L6-v2\" (fast), \"all-mpnet-base-v2\" (better)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# If there's an existing FAISS index saved, load it.\n",
    "if VECTORSTORE_PATH.exists():\n",
    "    print(\"Loading existing FAISS index from disk...\")\n",
    "    vectordb = FAISS.load_local(\n",
    "    str(VECTORSTORE_PATH),\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "else:\n",
    "    print(\"Creating FAISS index (this may take a while for many chunks)...\")\n",
    "    vectordb = FAISS.from_documents(texts, embeddings)\n",
    "    # save it\n",
    "    vectordb.save_local(str(VECTORSTORE_PATH))\n",
    "    print(\"Saved FAISS index to\", VECTORSTORE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a580755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4) Build a retriever and RAG chain\n",
    "# --------------------------\n",
    "# We want the retriever to return multiple docs (top_k) so LLM can synthesize across them.\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# Prompt template that instructs LLM to synthesize and cite sources.\n",
    "SYNTHESIS_PROMPT = \"\"\"You are an expert research assistant. Use the retrieved document chunks to answer the user's question.\n",
    "Important rules:\n",
    "1. Synthesize information from multiple retrieved documents when necessary.\n",
    "2. You are an academic summarizer. Use the following context to answer the question.\n",
    "3. If a fact comes from a specific page, include the page number or chunk metadata if available.\n",
    "4. If the retrieved documents conflict, present both views and say which papers support each view.\n",
    "5. Be concise and produce a final summary paragraph labeled \"Answer\".\n",
    "6. At the end, list the specific sources used (filename + short metadata) under \"Sources:\".\n",
    "\n",
    "Context (retrieved chunks):\n",
    "{context}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=SYNTHESIS_PROMPT, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# LLM selection - example OpenAI LLM (gpt-4 style) -- replace with your LLM if needed.\n",
    "if USE_OPENAI:\n",
    "    llm = OpenAI(temperature=0.0, model_name=\"gpt-4o\")  # or \"gpt-4\" if available; choose per access\n",
    "else:\n",
    "    # Placeholder: you should replace with your LLM implementation (HFHub, LlamaCpp, etc.)\n",
    "    llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.3)  # fallback placeholder\n",
    "\n",
    "# Create a RetrievalQA chain but customized to use our prompt and to return source documents\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",   # \"map_reduce\" or \"refine\" are alternatives for large contexts\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6aaa703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== QUERY ===\n",
      "Provide a concise summary of the main findings of the paper titled 'Introduction_sleeping.pdf'.\n",
      "\n",
      "--- ANSWER ---\n",
      "\n",
      "Unfortunately, the provided context does not include the full paper titled 'Introduction_sleeping.pdf'. However, based on the retrieved document chunks, I can provide a summary of the related findings:\n",
      "\n",
      "The role of sleep in memory consolidation is a subject of robust scientific inquiry. Research has shown that sleep serves to preserve memory from gradual decay (Jenkins & Dallenbach, 1924) and contributes to memory consolidation (Carskadon & Dement, 2000). Studies have also differentiated the effects of sleep on various types of learning, such as declarative vs. procedural (Plihal & Born, 1997).\n",
      "\n",
      "The paper 'Introduction_sleeping.pdf' is not explicitly mentioned in the provided context. However, the chunks suggest that the paper may discuss the effects of sleep on memory, particularly the potential weakness of sleep inertia and its effects on post-sleep performance.\n",
      "\n",
      "Answer:\n",
      "The main findings of the paper 'Introduction_sleeping.pdf' are not explicitly stated in the provided context. However, the related research suggests that sleep plays a crucial role in memory consolidation, and sleep inertia may be a potential weakness in sleep-related research.\n",
      "\n",
      "Sources:\n",
      "1. Jenkins, J.G., & Dallenbach, K.M. (1924). Obliviscence during sleep and waking. Am. J. Psychol., 35, 605-612.\n",
      "2. Carskadon, M. A., & Dement, W. C. (2000). Normal human sleep: an overview. Principles and practice of sleep medicine, 4, 13-23.\n",
      "3. Plihal, W., & Born, J. (1997). Effects of early and late nocturnal sleep on declarative memory. Nature, 387(6630), 266-268.\n",
      "4. Müller, G.E., & Pilzecker, A. (1900). Experimentelle beiträge zur lehre vom gedächtniss. Vol. 1, JA Barth.\n",
      "5. Tassi, P., & Muzet, A. (2000). Sleep inertia. Sleep Medicine Reviews, 4(4), 341-355.\n",
      "6. Achermann, P., & Werth, E. (1995). Sleep stage classification based on the dynamics of the electroencephalogram. IEEE Transactions on Biomedical Engineering, 42(11), 1111-1119.\n",
      "\n",
      "--- SOURCES (from retrieved chunks) ---\n",
      "1. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 1, length 178 chars)\n",
      "2. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 2, length 267 chars)\n",
      "3. papers\\Introduction_sleeping.pdf  (chunk id: 3, length 796 chars)\n",
      "4. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 4, length 982 chars)\n",
      "5. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 5, length 914 chars)\n",
      "6. papers\\Introduction_sleeping.pdf  (chunk id: 6, length 387 chars)\n",
      "\n",
      "\n",
      "=== QUERY ===\n",
      "What do the different authors agree on regarding memory consolidation?\n",
      "\n",
      "--- ANSWER ---\n",
      "\n",
      "The authors agree that memory consolidation is a complex process that may involve an extended time-course, and that sleep plays a crucial role in this process. They also agree that both the quantity and quality of sleep are essential for consolidating and integrating newly learnt information with existing knowledge (Walker & Stickgold, 2010; Winocur et al., 2010; Kumral et al., 2023; Leong & Cheng, 2005). Additionally, they suggest that the time-course of memory consolidation depends on the consistency of new information with existing long-term information (McClelland, 2013). However, the specific mechanisms and stages of memory consolidation are still being researched and debated.\n",
      "\n",
      "Sources:\n",
      "1. Walker, M. P., & Stickgold, R. (2010) - retrieved chunk\n",
      "2. Winocur, G., Moscovitch, M., & Bontempi, B. (2010) - retrieved chunk\n",
      "3. Kumral, D., Matzerath, A., Leonhart, R., & Schönauer, M. (2023) - retrieved chunk\n",
      "4. Leong, R. L. F., & Cheng, G. H. (2005) - retrieved chunk\n",
      "5. McClelland, J. L. (2013) - retrieved chunk\n",
      "\n",
      "--- SOURCES (from retrieved chunks) ---\n",
      "1. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 1, length 773 chars)\n",
      "2. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 2, length 972 chars)\n",
      "3. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 3, length 482 chars)\n",
      "4. papers\\Introduction_sleeping.pdf  (chunk id: 4, length 425 chars)\n",
      "5. papers\\Introduction_sleeping.pdf  (chunk id: 5, length 527 chars)\n",
      "6. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 6, length 783 chars)\n",
      "\n",
      "\n",
      "=== QUERY ===\n",
      "Compare the methodologies used in 'Elaine Tham (thesis)_corrections_final.pdf' and 'pone.0042191.pdf'.\n",
      "\n",
      "--- ANSWER ---\n",
      "\n",
      "Based on the retrieved document chunks, we can compare the methodologies used in 'Elaine Tham (thesis)_corrections_final.pdf' and 'pone.0042191.pdf'.\n",
      "\n",
      "Both studies appear to use similar experimental designs, with participants completing tasks involving word learning and memory. However, there are some differences in the specific methodologies used.\n",
      "\n",
      "In 'Elaine Tham (thesis)_corrections_final.pdf', the study uses a recognition task where participants are tested on their ability to remember word forms (see Table A2 in Appendix 1.2). The study also examines the effects of semantic distance and congruity on word learning, with participants completing tasks in both English and Malay.\n",
      "\n",
      "In contrast, 'pone.0042191.pdf' uses a paradigm that relies on processing beyond the recognition of word forms and is linked with the semantics of the newly learnt words (see 1.7 Summary and Thesis outline, Chapter 2: Overnight Sleep and Knowledge Integration in New Word Learning). The study also examines the effects of semantic distance and congruity on word learning, with participants completing tasks in both English and Malay.\n",
      "\n",
      "The main difference between the two studies appears to be the nature of the experimental tasks used. 'Elaine Tham (thesis)_corrections_final.pdf' uses a recognition task, while 'pone.0042191.pdf' uses a paradigm that requires processing beyond recognition and is linked with semantics.\n",
      "\n",
      "Sources:\n",
      "- 'Elaine Tham (thesis)_corrections_final.pdf' (Chapter 2: Overnight Sleep and Knowledge Integration in New Word Learning)\n",
      "- 'pone.0042191.pdf' (1.7 Summary and Thesis outline, Chapter 2: Overnight Sleep and Knowledge Integration in New Word Learning)\n",
      "\n",
      "--- SOURCES (from retrieved chunks) ---\n",
      "1. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 1, length 974 chars)\n",
      "2. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 2, length 995 chars)\n",
      "3. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 3, length 300 chars)\n",
      "4. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 4, length 966 chars)\n",
      "5. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 5, length 924 chars)\n",
      "6. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 6, length 563 chars)\n",
      "\n",
      "\n",
      "=== QUERY ===\n",
      "What open questions remain in this research area according to these papers?\n",
      "\n",
      "--- ANSWER ---\n",
      "\n",
      "According to the retrieved documents, open questions remain in this research area, particularly regarding the role of sleep in strengthening new word knowledge. The papers suggest that sleep may not be essential when semantics are directly related to task performance and there is a limited effect of semantic retrieval cues (SRC) (Chunk 5, no page number). However, when stronger automatic semantic access is required due to greater S-R mappings, a longer time-course that includes sleep is necessary (Chunk 6.3.1, page 180).\n",
      "\n",
      "Additionally, the papers highlight the need for further research on the circadian confounds that may affect the results (Chunk 6.6.2, page 202). The thesis also emphasizes the importance of future work in this area, including exploring the effects of different sleep schedules and durations on automaticity and memory consolidation (Chunk 6.7.2, page 204).\n",
      "\n",
      "Sources:\n",
      "1. Chunk 1.7 (Summary and Thesis outline, page 33)\n",
      "2. Chunk 2.1 (Chapter 2: Overnight Sleep and Knowledge Integration in New Word Learning, page 34)\n",
      "3. Chunk 6.1 (Thesis summary, page 174)\n",
      "4. Chunk 6.2 (Main findings, page 174)\n",
      "5. Chunk 6.3 (Sleep-associated changes in automaticity for new declarative memory, page 180)\n",
      "6. Chunk 6.6 (Circadian confounds, page 202)\n",
      "7. Chunk 6.7 (Conclusions and future work, page 203)\n",
      "\n",
      "--- SOURCES (from retrieved chunks) ---\n",
      "1. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 1, length 300 chars)\n",
      "2. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 2, length 950 chars)\n",
      "3. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 3, length 370 chars)\n",
      "4. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 4, length 317 chars)\n",
      "5. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 5, length 409 chars)\n",
      "6. papers\\Elaine Tham (thesis)_corrections_final.pdf  (chunk id: 6, length 256 chars)\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5) Test queries\n",
    "# --------------------------\n",
    "def run_query(q):\n",
    "    print(\"\\n\\n=== QUERY ===\")\n",
    "    print(q)\n",
    "    result = qa_chain({\"query\": q})\n",
    "    answer = result[\"result\"]\n",
    "    source_docs = result.get(\"source_documents\", [])\n",
    "    print(\"\\n--- ANSWER ---\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n--- SOURCES (from retrieved chunks) ---\")\n",
    "    for i, doc in enumerate(source_docs, 1):\n",
    "        src = doc.metadata.get(\"source\", \"unknown\")\n",
    "        # try to print some context about where the chunk came from\n",
    "        print(f\"{i}. {src}  (chunk id: {i}, length {len(doc.page_content)} chars)\")\n",
    "    return result\n",
    "\n",
    "# Example tests (you will replace titles/names by real filenames/titles)\n",
    "tests = [\n",
    "    \"Provide a concise summary of the main findings of the paper titled 'Introduction_sleeping.pdf'.\",\n",
    "    \"What do the different authors agree on regarding memory consolidation?\",\n",
    "    \"Compare the methodologies used in 'Elaine Tham (thesis)_corrections_final.pdf' and 'pone.0042191.pdf'.\",\n",
    "    \"What open questions remain in this research area according to these papers?\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    try:\n",
    "        run_query(t)\n",
    "    except Exception as e:\n",
    "        print(\"Error running query:\", e)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537df04c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
