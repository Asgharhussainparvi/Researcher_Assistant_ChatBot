ğŸ§  Research Assistant Chatbot using Groq API (RAG-based)

This project implements a Retrieval-Augmented Generation (RAG) chatbot powered by Groq API and LangChain, designed to answer questions by synthesizing information from multiple research paper PDFs.

It automatically loads all PDFs from a folder, processes and embeds their text, stores them in a FAISS vector database, and uses Groqâ€™s LLaMA-3 model to generate context-aware answers.

ğŸš€ Features

ğŸ“‚ Load multiple research article PDFs from a folder

ğŸ§© Chunk and embed text using HuggingFace embeddings (all-MiniLM-L6-v2)

âš¡ Fast and scalable vector retrieval via FAISS

ğŸ—£ï¸ Natural language Q&A using Groq LLaMA-3.1-8B-Instant

ğŸ§  Context-aware and citation-backed answers

ğŸ’¬ Interactive command-line chat interface

ğŸ—ï¸ Project Structure
ğŸ“¦ research-assistant-chatbot
â”œâ”€â”€ papers/                    # Folder containing all your research PDFs
â”œâ”€â”€ .env                       # Contains your Groq API key
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ simple_rag_chatbot.py                    # Main chatbot script
â””â”€â”€ README.md                   # Project documentation

âš™ï¸ Installation
1. Clone the repository
git clone https://github.com/<your-username>/research-assistant-chatbot.git
cd research-assistant-chatbot

2. Create and activate a virtual environment
python -m venv venv
source venv/bin/activate     # for Linux/Mac
venv\Scripts\activate        # for Windows

3. Install dependencies
pip install -r requirements.txt


Example requirements.txt:

langchain
langchain_groq
langchain_huggingface
langchain_community
python-dotenv
faiss-cpu
PyPDF2

4. Set up environment variables

Create a .env file in the project root:

GROQ_API_KEY=your_groq_api_key_here


You can obtain the API key from ğŸ‘‰ https://console.groq.com/

ğŸ“˜ Usage

Place all your research PDFs inside the papers/ folder.

Run the chatbot:

python main.py


Once loaded, youâ€™ll see:

RAG Chatbot Ready! You can now ask questions about your PDFs.
Type 'exit' to quit.


Start asking questions like:

You: What is the main contribution of the second paper?
You: Explain the methodology used in the dataset section.

ğŸ§© How It Works

PDF Loading: All PDFs are read and converted to text.

Text Chunking: Large text is split into manageable 1000-character chunks with overlaps.

Embedding: Each chunk is converted into vector form using HuggingFace embeddings.

Vector Store: FAISS stores these vectors for efficient similarity search.

Retrieval: Relevant chunks are retrieved based on user query.

Generation: Groqâ€™s LLaMA-3 model synthesizes a concise and factual response using the context.

ğŸ§  Example Output
You: What is the objective of the proposed model in these papers?
Thinking...

Answer:
The main objective of the proposed models across the papers is to improve contextual understanding and performance in domain-specific text classification tasks using lightweight transformer architectures.

============================================================

ğŸ§‘â€ğŸ’» Technologies Used

Python 3.10+

LangChain

Groq API (LLaMA-3.1-8B-Instant)

FAISS

HuggingFace Sentence Embeddings

PyPDFLoader

ğŸ“Œ Future Enhancements

Add a Streamlit web interface

Integrate citation retrieval and document references

Enable long-term memory for multi-session chat

Support additional document types (e.g., Word, HTML, CSV)

ğŸ§‘â€ğŸ”¬ Author

Asghar Hussain
AI Researcher | Software Engineering Student | AI Intern
ğŸ”— GitHub

ğŸ’¡ Passionate about AI, RAG systems, and scientific research automation.